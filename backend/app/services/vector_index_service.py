# app/services/vector_index_service.py
# this file is mongodb -> chromadb
# add new fields to mongodb: vector_indexed, embedding_model
# Embedding Pipeline

import os
from typing import List, Dict, Optional

from pymongo import MongoClient
from sentence_transformers import SentenceTransformer
import chromadb

# ==== Configuration ====
MONGO_URI = os.getenv("MONGO_URI", "mongodb://mongodb:27017")
DB_NAME = os.getenv("MONGO_DB", "badger_db")
COLL_NAME = os.getenv("MONGO_COLL", "papers")

CHROMA_PERSIST_DIR = "/app/chroma_data"
CHROMA_COLLECTION_NAME = "papers_embeddings"

EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
BATCH_SIZE = 64


class VectorIndexService:
    """Handles embedding generation and ChromaDB indexing."""

    def __init__(self):
        # ----- MongoDB -----
        self.mongo_client = MongoClient(MONGO_URI)
        self.mongo_coll = self.mongo_client[DB_NAME][COLL_NAME]

        # ----- Embedding model -----
        print("Loading embedding model:", EMBEDDING_MODEL)
        self.model = SentenceTransformer(EMBEDDING_MODEL)

        # ----- ChromaDB -----
        print("Connecting to ChromaDB...")
        self.chroma_client = chromadb.PersistentClient(path=CHROMA_PERSIST_DIR)

        self.chroma_coll = self.chroma_client.get_or_create_collection(
            name=CHROMA_COLLECTION_NAME,
            metadata={"hnsw:space": "cosine"}  # cosine è·ç¦»
        )

        print("âœ… VectorIndexService initialized.")

    # ========= Load unindexed docs =========
    def load_unindexed_papers(self, limit: Optional[int] = None) -> List[Dict]:
        """
        ä»Ž MongoDB é‡ŒåŠ è½½è¿˜æ²¡æœ‰è¢«å‘é‡åŒ–çš„è®ºæ–‡ã€‚
        è§„åˆ™ï¼š vector_indexed != True çš„éƒ½ç®—â€œæœªç´¢å¼•â€
        ï¼ˆåŒ…æ‹¬å­—æ®µä¸å­˜åœ¨ / false / nullï¼‰
        """
        query = {"vector_indexed": {"$ne": True}}
        cursor = self.mongo_coll.find(query)

        if limit:
            cursor = cursor.limit(limit)

        papers = list(cursor)
        print(f"Loaded {len(papers)} unindexed papers from MongoDB")
        return papers

    # ========= Build text for embedding =========
    @staticmethod
    def build_text(paper: Dict) -> str:
        """ä½¿ç”¨ title + summary æž„å»º embedding è¾“å…¥æ–‡æœ¬ã€‚"""
        title = paper.get("title", "").strip()
        summary = paper.get("summary", "").strip()
        if not title and not summary:
            return ""
        return f"Title: {title}\n\nAbstract: {summary}"

    # ========= Embed & Write to Chroma =========
    def embed_and_index(self, papers: List[Dict]) -> int:
        if not papers:
            print("âš ï¸ No papers to index in this batch.")
            return 0

        # mongo_ids ç”¨æ¥æ›´æ–° Mongo
        # chroma_ids å¿…é¡»æ˜¯ strï¼Œç”¨æ¥å†™å…¥ Chroma
        mongo_ids = []
        chroma_ids = []
        texts = []
        metadatas = []

        for paper in papers:
            text = self.build_text(paper)
            if not text.strip():
                continue

            mongo_id = paper["_id"]
            chroma_id = str(mongo_id)

            mongo_ids.append(mongo_id)
            chroma_ids.append(chroma_id)
            texts.append(text)

            metadatas.append({
                "arxiv_id": paper.get("arxiv_id"),
                "title": paper.get("title"),
                "primary_category": paper.get("primary_category"),
            })

        if not texts:
            print("âš ï¸ All papers in this batch had empty text, skip.")
            return 0

        print(f"ðŸ§  Embedding {len(texts)} papers...")
        embeddings = self.model.encode(
            texts,
            show_progress_bar=True,
            batch_size=BATCH_SIZE
        )

        print("ðŸ’¾ Writing embeddings to Chroma...")
        self.chroma_coll.add(
            ids=chroma_ids,
            documents=texts,
            metadatas=metadatas,
            embeddings=embeddings
        )

        # æ›´æ–° Mongoï¼Œæ‰“æ ‡è®°
        for mid in mongo_ids:
            self.mongo_coll.update_one(
                {"_id": mid},
                {"$set": {
                    "vector_indexed": True,
                    "embedding_model": EMBEDDING_MODEL
                }}
            )

        print(f"âœ… Indexed {len(chroma_ids)} papers in this batch.")
        return len(chroma_ids)

    # ========= Entry point =========
    def run_indexing(self, limit: Optional[int] = None, batch_size: int = BATCH_SIZE) -> int:
        papers = self.load_unindexed_papers(limit)
        if not papers:
            print("All papers indexed.")
            return 0

        total = len(papers)
        print(f"\nðŸš€ Starting vector indexing for {total} papers...")

        indexed_total = 0

        for i in range(0, total, batch_size):
            batch = papers[i:i + batch_size]
            print(f"\nðŸ“¦ Batch {i // batch_size + 1}")
            indexed_total += self.embed_and_index(batch)

        print("\nðŸŽ‰ Completed")
        print(f"Total indexed: {indexed_total}")
        print(f"Remaining (unindexed, by this run): {total - indexed_total}")

        return indexed_total


# Singleton
vector_index_service = VectorIndexService()